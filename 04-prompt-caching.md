# Prompt Caching: Optimizaci√≥n de Latencia y Costos

## üìã Tabla de Contenidos
- [Introducci√≥n](#introducci√≥n)
- [C√≥mo Funciona](#c√≥mo-funciona)
- [Estructuraci√≥n de Prompts](#estructuraci√≥n-de-prompts)
- [Requisitos](#requisitos)
- [Qu√© se Puede Cachear](#qu√©-se-puede-cachear)
- [Mejores Pr√°cticas](#mejores-pr√°cticas)
- [Preguntas Frecuentes](#preguntas-frecuentes)

## Introducci√≥n

Reduce la latencia y el costo con **prompt caching**.

Los prompts de modelos a menudo contienen contenido repetitivo, como system prompts e instrucciones comunes. OpenAI enruta las solicitudes de API a servidores que procesaron recientemente el mismo prompt, haci√©ndolo m√°s barato y r√°pido que procesar un prompt desde cero. Esto puede reducir la latencia hasta en un **80%** y el costo hasta en un **75%**.

El **Prompt Caching** funciona autom√°ticamente en todas tus solicitudes de API (sin cambios de c√≥digo requeridos) y no tiene tarifas adicionales asociadas. Est√° habilitado para todos los [modelos](/docs/models) recientes, gpt-4o y m√°s nuevos.

Esta gu√≠a describe c√≥mo funciona el prompt caching en detalle, para que puedas optimizar tus prompts para menor latencia y costo.

## Estructuraci√≥n de Prompts

Los **cache hits** solo son posibles para coincidencias exactas de prefijo dentro de un prompt. Para aprovechar los beneficios del caching, coloca contenido est√°tico como instrucciones y ejemplos al **inicio** de tu prompt, y pon contenido variable, como informaci√≥n espec√≠fica del usuario, al **final**. Esto tambi√©n se aplica a im√°genes y herramientas, que deben ser id√©nticas entre solicitudes.

![Visualizaci√≥n de Prompt Caching](https://openaidevs.retool.com/api/file/8593d9bb-4edb-4eb6-bed9-62bfb98db5ee)

### Ejemplo de Estructuraci√≥n √ìptima

```
‚úÖ ESTRUCTURA RECOMENDADA:
[Contenido est√°tico - instrucciones, ejemplos, system prompt]
[Contenido variable - datos del usuario, consultas espec√≠ficas]

‚ùå ESTRUCTURA NO OPTIMIZADA:
[Contenido variable]
[Contenido est√°tico]
[Contenido variable]
```

## C√≥mo Funciona

El caching se habilita autom√°ticamente para prompts de **1024 tokens** o m√°s. Cuando haces una solicitud de API, ocurren los siguientes pasos:

### 1. Cache Routing

- Las solicitudes se enrutan a una m√°quina bas√°ndose en un hash del prefijo inicial del prompt. El hash t√≠picamente usa los primeros **256 tokens**, aunque la longitud exacta var√≠a dependiendo del modelo.
- Si proporcionas el par√°metro [`prompt_cache_key`](/docs/api-reference/responses/create#responses-create-prompt_cache_key), se combina con el hash del prefijo, permiti√©ndote influir en el enrutamiento y mejorar las tasas de cache hit. Esto es especialmente beneficioso cuando muchas solicitudes comparten prefijos largos y comunes.
- Si las solicitudes para la misma combinaci√≥n de prefijo y `prompt_cache_key` exceden cierta tasa (aproximadamente **15 solicitudes por minuto**), algunas pueden desbordarse y ser enrutadas a m√°quinas adicionales, reduciendo la efectividad del cache.

### 2. Cache Lookup

El sistema verifica si la porci√≥n inicial (prefijo) de tu prompt existe en el cache de la m√°quina seleccionada.

### 3. Cache Hit

Si se encuentra un prefijo coincidente, el sistema usa el resultado cacheado. Esto disminuye significativamente la latencia y reduce los costos.

### 4. Cache Miss

Si no se encuentra un prefijo coincidente, el sistema procesa tu prompt completo, cacheando el prefijo despu√©s en esa m√°quina para solicitudes futuras.

### Duraci√≥n del Cache

Los prefijos cacheados generalmente permanecen activos durante **5 a 10 minutos** de inactividad. Sin embargo, durante per√≠odos de baja demanda, los caches pueden persistir hasta **una hora**.

## Requisitos

### Tokens M√≠nimos

El caching est√° disponible para prompts que contienen **1024 tokens** o m√°s, con cache hits ocurriendo en incrementos de **128 tokens**. Por lo tanto, el n√∫mero de tokens cacheados en una solicitud siempre caer√° dentro de la siguiente secuencia: 1024, 1152, 1280, 1408, y as√≠ sucesivamente, dependiendo de la longitud del prompt.

### Monitoreo de Cache

Todas las solicitudes, incluyendo aquellas con menos de 1024 tokens, mostrar√°n un campo `cached_tokens` en el objeto `usage.prompt_tokens_details` del [Response object](/docs/api-reference/responses/object) o [Chat object](/docs/api-reference/chat/object) indicando cu√°ntos de los tokens del prompt fueron un cache hit. Para solicitudes bajo 1024 tokens, `cached_tokens` ser√° cero.

#### Ejemplo de Respuesta con Cache

```json
{
  "usage": {
    "prompt_tokens": 2006,
    "completion_tokens": 300,
    "total_tokens": 2306,
    "prompt_tokens_details": {
      "cached_tokens": 1920
    },
    "completion_tokens_details": {
      "reasoning_tokens": 0,
      "accepted_prediction_tokens": 0,
      "rejected_prediction_tokens": 0
    }
  }
}
```

## Qu√© se Puede Cachear

### ‚úÖ Elementos Cacheables

| Elemento | Descripci√≥n | Consideraciones |
|----------|-------------|-----------------|
| **Messages** | El array completo de mensajes, incluyendo system, user y assistant interactions | Debe ser id√©ntico entre solicitudes |
| **Images** | Im√°genes incluidas en mensajes de usuario, como links o datos base64 | Aseg√∫rate de que el par√°metro `detail` est√© configurado id√©nticamente |
| **Tool use** | Tanto el array de mensajes como la lista de `tools` disponibles | Contribuyen al requisito m√≠nimo de 1024 tokens |
| **Structured outputs** | El schema de salida estructurada sirve como prefijo al system message | Puede ser cacheado |

### üîÑ Optimizaci√≥n de Cache

- **Contenido est√°tico**: Instrucciones, ejemplos, system prompts
- **Contenido din√°mico**: Datos espec√≠ficos del usuario, consultas variables
- **Herramientas**: Definiciones de tools que no cambian frecuentemente

## Mejores Pr√°cticas

### üéØ Estrategias de Optimizaci√≥n

#### 1. Estructura de Prompts
```
‚úÖ HACER:
- Colocar contenido est√°tico o repetido al INICIO
- Colocar contenido din√°mico y espec√≠fico del usuario al FINAL
- Mantener consistencia en la estructura entre solicitudes similares

‚ùå EVITAR:
- Mezclar contenido est√°tico y din√°mico
- Cambiar la estructura frecuentemente
- Colocar contenido variable al inicio
```

#### 2. Uso de prompt_cache_key
- Usa el par√°metro [`prompt_cache_key`](/docs/api-reference/responses/create#responses-create-prompt_cache_key) consistentemente en solicitudes que comparten prefijos comunes
- Selecciona una granularidad que mantenga cada combinaci√≥n √∫nica de prefijo-`prompt_cache_key` por debajo de **15 solicitudes por minuto** para evitar cache overflow

#### 3. Monitoreo de Performance
- **M√©tricas clave a monitorear**:
  - Tasas de cache hit
  - Latencia
  - Proporci√≥n de tokens cacheados
  - Costos por solicitud

#### 4. Mantenimiento de Cache
- Mant√©n un flujo constante de solicitudes con prefijos de prompt id√©nticos
- Minimiza las cache evictions para maximizar los beneficios del caching
- Monitorea el rendimiento regularmente

### üìä M√©tricas de Rendimiento

| M√©trica | Objetivo | Monitoreo |
|---------|----------|-----------|
| **Cache Hit Rate** | >70% | Porcentaje de solicitudes que usan cache |
| **Latency Reduction** | 60-80% | Comparaci√≥n con solicitudes sin cache |
| **Cost Reduction** | 50-75% | Reducci√≥n en tokens procesados |
| **Cache Tokens** | M√°ximo posible | Tokens cacheados por solicitud |

## Preguntas Frecuentes

### üîí Privacidad y Seguridad

#### 1. ¬øC√≥mo se mantiene la privacidad de datos para los caches?

Los prompt caches **no se comparten entre organizaciones**. Solo los miembros de la misma organizaci√≥n pueden acceder a caches de prompts id√©nticos.

#### 2. ¬øEl Prompt Caching afecta la generaci√≥n de tokens de salida o la respuesta final de la API?

**No**. El Prompt Caching no influye en la generaci√≥n de tokens de salida o la respuesta final proporcionada por la API. Independientemente de si se usa caching, la salida generada ser√° id√©ntica. Esto es porque solo el prompt en s√≠ se cachea, mientras que la respuesta real se calcula de nuevo cada vez bas√°ndose en el prompt cacheado.

### üõ†Ô∏è Gesti√≥n y Control

#### 3. ¬øHay alguna forma de limpiar manualmente el cache?

La limpieza manual del cache **no est√° disponible actualmente**. Los prompts que no han sido encontrados recientemente se limpian autom√°ticamente del cache. Las cache evictions t√≠picas ocurren despu√©s de **5-10 minutos** de inactividad, aunque a veces duran hasta un m√°ximo de **una hora** durante per√≠odos de baja demanda.

#### 4. ¬øSe espera que pague extra por escribir en Prompt Caching?

**No**. El caching ocurre autom√°ticamente, sin acci√≥n expl√≠cita necesaria ni costo extra para usar la funcionalidad de caching.

### üìà L√≠mites y Restricciones

#### 5. ¬øLos prompts cacheados contribuyen a los l√≠mites de tasa TPM?

**S√≠**, ya que el caching no afecta los rate limits.

#### 6. ¬øEl descuento para Prompt Caching est√° disponible en Scale Tier y Batch API?

El descuento para Prompt Caching **no est√° disponible** en Batch API pero **s√≠ est√° disponible** en Scale Tier. Con Scale Tier, cualquier token que se desborde a la API compartida tambi√©n ser√° elegible para caching.

#### 7. ¬øEl Prompt Caching funciona en solicitudes Zero Data Retention?

**S√≠**, el Prompt Caching es compatible con las pol√≠ticas existentes de Zero Data Retention.

---

> **Nota**: El Prompt Caching es una funcionalidad autom√°tica que optimiza el rendimiento sin requerir cambios en tu c√≥digo. Para maximizar los beneficios, enf√≥cate en estructurar tus prompts de manera consistente y monitorear las m√©tricas de rendimiento.

### Pr√≥ximos Pasos

1. **Revisa** la estructura de tus prompts actuales
2. **Implementa** las mejores pr√°cticas de estructuraci√≥n
3. **Monitorea** las m√©tricas de cache hit
4. **Optimiza** bas√°ndote en los resultados
5. **Considera** usar `prompt_cache_key` para casos espec√≠ficos

---

*Esta gu√≠a se actualiza regularmente con las mejores pr√°cticas emergentes en prompt caching.*
